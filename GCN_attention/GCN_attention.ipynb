{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    train_file = './stanford-covid-vaccine/train.json'\n",
    "    test_file = './stanford-covid-vaccine/test.json'\n",
    "    pretrain_dir = './GCN_lstm_patience10'\n",
    "    sample_submission = './stanford-covid-vaccine/sample_submission.csv'\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    n_epoch = 200\n",
    "    n_split = 5\n",
    "    K = 1 # number of aggregation loop (also means number of GCN layers)\n",
    "    gcn_agg = 'mean' # aggregator function: mean, conv, lstm, pooling\n",
    "    filter_noise = True\n",
    "    patience= 10\n",
    "    seed = 1234\n",
    "    loss_weights = torch.tensor([0.3,0.3,0.05,0.3,0.05]).cuda()\n",
    "    dropout_rate = 0.4\n",
    "    attention_dropout = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    '''\n",
    "    Implementation of one layer of GraphSAGE\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, aggregator=config.gcn_agg):\n",
    "        super(GCN, self).__init__()\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "        if aggregator == 'mean':\n",
    "            linear_input_dim = input_dim * 2\n",
    "        elif aggregator == 'conv':\n",
    "            linear_input_dim = input_dim\n",
    "#         elif aggregator == 'pooling':\n",
    "#             linear_input_dim = input_dim * 2\n",
    "#             self.linear_pooling = nn.Linear(input_dim, input_dim)\n",
    "        elif aggregator == 'lstm':\n",
    "            self.lstm_hidden = 64\n",
    "            linear_input_dim = input_dim + self.lstm_hidden\n",
    "            self.lstm_agg = nn.LSTM(input_dim, self.lstm_hidden, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.linear_gcn = nn.Linear(in_features=linear_input_dim, out_features=output_dim)\n",
    "        \n",
    "    def forward(self, input_, adj_matrix):\n",
    "        if self.aggregator == 'conv':\n",
    "            # set elements in diagonal of adj matrix to 1 with conv aggregator\n",
    "            idx = torch.arange(0, adj_matrix.shape[-1], out=torch.LongTensor())\n",
    "            adj_matrix[:, idx, idx] = 1\n",
    "            \n",
    "        adj_matrix = adj_matrix.type(torch.float32)\n",
    "        sum_adj = torch.sum(adj_matrix, axis=2)\n",
    "        sum_adj[sum_adj==0] = 1\n",
    "        \n",
    "        if self.aggregator == 'mean' or self.aggregator == 'conv':\n",
    "            feature_agg = torch.bmm(adj_matrix, input_)\n",
    "            feature_agg = feature_agg / sum_adj.unsqueeze(dim=2)\n",
    "            \n",
    "#         elif self.aggregator == 'pooling':\n",
    "#             feature_pooling = self.linear_pooling(input_)\n",
    "#             feature_agg = torch.sigmoid(feature_pooling)\n",
    "#             feature_agg = torch.bmm(adj_matrix, feature_agg)\n",
    "#             feature_agg = feature_agg / sum_adj.unsqueeze(dim=2)\n",
    "\n",
    "        elif self.aggregator == 'lstm':\n",
    "            feature_agg = torch.zeros(input_.shape[0], input_.shape[1], self.lstm_hidden).cuda()\n",
    "            for i in range(adj_matrix.shape[1]):\n",
    "                neighbors = adj_matrix[:, i, :].unsqueeze(2) * input_\n",
    "                _, hn = self.lstm_agg(neighbors)\n",
    "                feature_agg[:, i, :] = torch.squeeze(hn[0], 0)\n",
    "                \n",
    "        if self.aggregator != 'conv':\n",
    "            feature_cat = torch.cat((input_, feature_agg), axis=2)\n",
    "        else:\n",
    "            feature_cat = feature_agg\n",
    "                \n",
    "        feature = torch.sigmoid(self.linear_gcn(feature_cat))\n",
    "        feature = feature / torch.norm(feature, p=2, dim=2).unsqueeze(dim=2)\n",
    "        \n",
    "        return feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://www.kaggle.com/gopidurgaprasad/covid-ae-gnn-attn-cnn-pytorch\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_channels, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.x_Q = nn.Conv1d(in_channels=in_channels, out_channels=self.output_dim, kernel_size=1, padding=1//2).cuda()\n",
    "        self.x_K = nn.Conv1d(in_channels=in_channels, out_channels=self.output_dim, kernel_size=1, padding=1//2).cuda()\n",
    "        self.x_V = nn.Conv1d(in_channels=in_channels, out_channels=self.output_dim, kernel_size=1, padding=1//2).cuda()\n",
    "\n",
    "\n",
    "    def forward(self, feature):\n",
    "        feature = feature.permute(0,2,1)\n",
    "\n",
    "        x_Q = self.x_Q(feature)  # (N, value_len, heads, head_dim)\n",
    "        x_K = self.x_K(feature)  # (N, key_len, heads, head_dim)\n",
    "        x_V = self.x_V(feature)  # (N, query_len, heads, heads_dim)\n",
    "        \n",
    "        x_Q = x_Q.permute(0, 2, 1)\n",
    "        x_K = x_K.permute(0, 2, 1)\n",
    "        x_V = x_V.permute(0, 2, 1)\n",
    "\n",
    "        res = torch.einsum(\"nqd,nkd->nqk\", [x_Q, x_K])\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(res / (self.output_dim ** (1 / 2)), dim=2)\n",
    "\n",
    "\n",
    "        attention = torch.einsum(\"nql,nld->nqd\", [attention, x_V])\n",
    "        return attention\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, in_channels, output_dim, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(output_dim).cuda()\n",
    "\n",
    "        self.n_factor_head = output_dim // n_head\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(in_channels, self.n_factor_head) for _ in range(n_head)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_dropout).cuda()\n",
    "        \n",
    "    def forward(self, feature):     \n",
    "        att_out = []\n",
    "        for head in self.heads:\n",
    "            out = head(feature)\n",
    "            att_out.append(out)\n",
    "\n",
    "        att = torch.cat(att_out, dim=2)\n",
    "        #feature = feature + att\n",
    "        att = self.norm1(att)\n",
    "        att = self.dropout(att)\n",
    "    \n",
    "        return att\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_embedding=14, seq_len=107, pred_len=68, dropout=config.dropout_rate, \n",
    "                 embed_dim=100, hidden_dim=128, K=1, aggregator='mean'):\n",
    "        '''\n",
    "        K: number of GCN layers\n",
    "        aggregator: type of aggregator function\n",
    "        '''\n",
    "        print(\"using aggregate function %s\"%aggregator)\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.pred_len = pred_len\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=num_embedding, \n",
    "                                      embedding_dim=embed_dim)\n",
    "        \n",
    "        self.gcn = nn.ModuleList([GCN(3 * embed_dim, 3 * embed_dim, aggregator=aggregator) for i in range(K)])\n",
    "        self.MultiHeadAttention1 = MultiHeadAttention(in_channels=300,output_dim=256,n_head=4)\n",
    "        \n",
    "        self.gru_layer = nn.GRU(input_size=256, \n",
    "                          hidden_size=hidden_dim, \n",
    "                          num_layers=3, \n",
    "                          batch_first=True, \n",
    "                          dropout=config.dropout_rate, \n",
    "                          bidirectional=True)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=2 * hidden_dim, \n",
    "                                out_features=5)\n",
    "        \n",
    "    def forward(self, input_, adj_matrix):\n",
    "        #embedding\n",
    "        embedding = self.embedding_layer(input_)\n",
    "        embedding = torch.reshape(embedding, (-1, embedding.shape[1], embedding.shape[2] * embedding.shape[3]))\n",
    "    \n",
    "        #gcn\n",
    "        gcn_feature = embedding\n",
    "        for gcn_layer in self.gcn:\n",
    "            gcn_feature = gcn_layer(gcn_feature, adj_matrix)\n",
    "        attentin_feature = self.MultiHeadAttention1(gcn_feature)\n",
    "        \n",
    "        #gru\n",
    "        gru_output, gru_hidden = self.gru_layer(attentin_feature)\n",
    "        truncated = gru_output[:, :self.pred_len]\n",
    "        \n",
    "        output = self.linear_layer(truncated)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n",
    "\n",
    "def get_couples(structure):\n",
    "    \"\"\"\n",
    "    For each closing parenthesis, I find the matching opening one and store their index in the couples list.\n",
    "    The assigned list is used to keep track of the assigned opening parenthesis\n",
    "    \"\"\"\n",
    "    opened = [idx for idx, i in enumerate(structure) if i == '(']\n",
    "    closed = [idx for idx, i in enumerate(structure) if i == ')']\n",
    "\n",
    "    assert len(opened) == len(closed)\n",
    "    assigned = []\n",
    "    couples = []\n",
    "\n",
    "    for close_idx in closed:\n",
    "        for open_idx in opened:\n",
    "            if open_idx < close_idx:\n",
    "                if open_idx not in assigned:\n",
    "                    candidate = open_idx\n",
    "            else:\n",
    "                break\n",
    "        assigned.append(candidate)\n",
    "        couples.append([candidate, close_idx])\n",
    "        \n",
    "    assert len(couples) == len(opened)\n",
    "    \n",
    "    return couples\n",
    "\n",
    "def build_matrix(couples, size):\n",
    "    mat = np.zeros((size, size))\n",
    "    \n",
    "    for i in range(size):  # neigbouring bases are linked as well\n",
    "        if i < size - 1:\n",
    "            mat[i, i + 1] = 1\n",
    "        if i > 0:\n",
    "            mat[i, i - 1] = 1\n",
    "    \n",
    "    for i, j in couples:\n",
    "        mat[i, j] = 1\n",
    "        mat[j, i] = 1\n",
    "        \n",
    "    return mat\n",
    "\n",
    "def convert_to_adj(structure):\n",
    "    couples = get_couples(structure)\n",
    "    mat = build_matrix(couples, len(structure))\n",
    "    return mat\n",
    "\n",
    "def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n",
    "    inputs = np.transpose(\n",
    "        np.array(\n",
    "            df[cols]\n",
    "            .applymap(lambda seq: [token2int[x] for x in seq])\n",
    "            .values\n",
    "            .tolist()\n",
    "        ),\n",
    "        (0, 2, 1)\n",
    "    )\n",
    "    \n",
    "    adj_matrix = np.array(df['structure'].apply(convert_to_adj).values.tolist())\n",
    "    \n",
    "    return inputs, adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(config.train_file, lines=True)\n",
    "\n",
    "if config.filter_noise:\n",
    "    train = train[train.signal_to_noise > 1]\n",
    "    \n",
    "test = pd.read_json(config.test_file, lines=True)\n",
    "sample_df = pd.read_csv(config.sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_adj = preprocess_inputs(train)\n",
    "train_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.long)\n",
    "train_adj = torch.tensor(train_adj, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epoch, model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    train_loss = AverageMeter()\n",
    "    \n",
    "    for index, (input_, adj, label) in enumerate(train_loader):\n",
    "        input_ = input_.cuda()\n",
    "        adj = adj.cuda()\n",
    "        label = label.cuda()\n",
    "        preds = model(input_, adj)\n",
    "    \n",
    "        loss = criterion(preds, label, config.loss_weights)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.update(loss.item())\n",
    "    \n",
    "    print(f\"Train loss {train_loss.avg}\")\n",
    "    return train_loss.avg\n",
    "    \n",
    "def eval_fn(epoch, model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    eval_loss = AverageMeter()\n",
    "    \n",
    "    for index, (input_, adj, label) in enumerate(valid_loader):\n",
    "        input_ = input_.cuda()\n",
    "        adj = adj.cuda()\n",
    "        label = label.cuda()\n",
    "        preds = model(input_, adj)\n",
    "        \n",
    "        loss = criterion(preds, label, config.loss_weights)\n",
    "        eval_loss.update(loss.item())\n",
    "    \n",
    "    print(f\"Valid loss {eval_loss.avg}\")\n",
    "    return eval_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(pred_, label, weight):\n",
    "    num = pred_.shape[0] * pred_.shape[1]\n",
    "    return torch.sum(weight * (pred_ - label) ** 2)/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold, train_loader, valid_loader):\n",
    "    model = Net(K=config.K, aggregator=config.gcn_agg)\n",
    "    model.cuda()\n",
    "    criterion = weighted_mse_loss\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=config.learning_rate, weight_decay=0.0)\n",
    "    \n",
    "    eval_loss_increase_step = 0\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    for epoch in range(config.n_epoch):\n",
    "        print('#################')\n",
    "        print('###Epoch:', epoch)\n",
    "        \n",
    "        train_loss = train_fn(epoch, model, train_loader, criterion, optimizer)\n",
    "        eval_loss = eval_fn(epoch, model, valid_loader, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        eval_losses.append(eval_loss)\n",
    "        \n",
    "        # check if should early stop\n",
    "        if len(eval_losses) == 2:\n",
    "            previous_eval_loss = eval_losses[0]\n",
    "        if len(eval_losses) >= 2:\n",
    "            if eval_loss > previous_eval_loss:\n",
    "                eval_loss_increase_step += 1\n",
    "            else: \n",
    "                # save the model if it is better than previous step\n",
    "                torch.save(model.state_dict(), f'{config.pretrain_dir}/gcn_gru_{fold}.pt')\n",
    "                eval_loss_increase_step = 0\n",
    "                previous_eval_loss=eval_loss\n",
    "                \n",
    "        if eval_loss_increase_step >= config.patience:\n",
    "            print(\"early stop the model at Epoch: \", epoch)\n",
    "            del model\n",
    "            break\n",
    "            \n",
    "        \n",
    "#     torch.save(model.state_dict(), f'{config.pretrain_dir}/gcn_gru_{fold}.pt')\n",
    "    return train_losses, eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using aggregate function mean\n",
      "#################\n",
      "###Epoch: 0\n",
      "Train loss 0.26818221034827056\n",
      "Valid loss 0.23226902740342276\n",
      "#################\n",
      "###Epoch: 1\n",
      "Train loss 0.21601521030620294\n",
      "Valid loss 0.2168240866490773\n",
      "#################\n",
      "###Epoch: 2\n",
      "Train loss 0.20743493735790253\n",
      "Valid loss 0.21618653408118657\n",
      "#################\n",
      "###Epoch: 3\n",
      "Train loss 0.20461730051923682\n",
      "Valid loss 0.2094889602490834\n",
      "#################\n",
      "###Epoch: 4\n",
      "Train loss 0.1974244796567493\n",
      "Valid loss 0.19309456646442413\n",
      "#################\n",
      "###Epoch: 5\n",
      "Train loss 0.17378156107884865\n",
      "Valid loss 0.17461703079087393\n",
      "#################\n",
      "###Epoch: 6\n",
      "Train loss 0.16093984025496025\n",
      "Valid loss 0.16940865346363612\n",
      "#################\n",
      "###Epoch: 7\n",
      "Train loss 0.15238899323675367\n",
      "Valid loss 0.16231838294437953\n",
      "#################\n",
      "###Epoch: 8\n",
      "Train loss 0.14779026254459662\n",
      "Valid loss 0.1565954259463719\n",
      "#################\n",
      "###Epoch: 9\n",
      "Train loss 0.14257763381357547\n",
      "Valid loss 0.14463493440832412\n",
      "#################\n",
      "###Epoch: 10\n",
      "Train loss 0.13623935894833672\n",
      "Valid loss 0.14157323752130782\n",
      "#################\n",
      "###Epoch: 11\n",
      "Train loss 0.1354387076916518\n",
      "Valid loss 0.14348155898707254\n",
      "#################\n",
      "###Epoch: 12\n",
      "Train loss 0.13137711529378537\n",
      "Valid loss 0.13853469916752406\n",
      "#################\n",
      "###Epoch: 13\n",
      "Train loss 0.12986860755417082\n",
      "Valid loss 0.13469722654138291\n",
      "#################\n",
      "###Epoch: 14\n",
      "Train loss 0.12660618595502995\n",
      "Valid loss 0.13273066069398606\n",
      "#################\n",
      "###Epoch: 15\n",
      "Train loss 0.1250031237249021\n",
      "Valid loss 0.13676799408027104\n",
      "#################\n",
      "###Epoch: 16\n",
      "Train loss 0.1240368002542743\n",
      "Valid loss 0.13556012085505895\n",
      "#################\n",
      "###Epoch: 17\n",
      "Train loss 0.12106197658512327\n",
      "Valid loss 0.1289182390485491\n",
      "#################\n",
      "###Epoch: 18\n",
      "Train loss 0.12029142650189223\n",
      "Valid loss 0.12278893057789121\n",
      "#################\n",
      "###Epoch: 19\n",
      "Train loss 0.11703225997862993\n",
      "Valid loss 0.12021766496556145\n",
      "#################\n",
      "###Epoch: 20\n",
      "Train loss 0.11477426853444841\n",
      "Valid loss 0.11940359004906245\n",
      "#################\n",
      "###Epoch: 21\n",
      "Train loss 0.11153912627034718\n",
      "Valid loss 0.1176590131861823\n",
      "#################\n",
      "###Epoch: 22\n",
      "Train loss 0.1109079310187587\n",
      "Valid loss 0.11061986322913851\n",
      "#################\n",
      "###Epoch: 23\n",
      "Train loss 0.10619227201850326\n",
      "Valid loss 0.1097928700702531\n",
      "#################\n",
      "###Epoch: 24\n",
      "Train loss 0.10547000335322486\n",
      "Valid loss 0.10734073391982488\n",
      "#################\n",
      "###Epoch: 25\n",
      "Train loss 0.10220822471159476\n",
      "Valid loss 0.10499263874122075\n",
      "#################\n",
      "###Epoch: 26\n",
      "Train loss 0.10031583436109402\n",
      "Valid loss 0.10201199672051839\n",
      "#################\n",
      "###Epoch: 27\n",
      "Train loss 0.09958092785543865\n",
      "Valid loss 0.1023114025592804\n",
      "#################\n",
      "###Epoch: 28\n",
      "Train loss 0.0970969420892221\n",
      "Valid loss 0.10165246043886457\n",
      "#################\n",
      "###Epoch: 29\n",
      "Train loss 0.09584913816716936\n",
      "Valid loss 0.09695940784045629\n",
      "#################\n",
      "###Epoch: 30\n",
      "Train loss 0.09309636387560102\n",
      "Valid loss 0.09537827755723681\n",
      "#################\n",
      "###Epoch: 31\n",
      "Train loss 0.09169524990850025\n",
      "Valid loss 0.09381626759256635\n",
      "#################\n",
      "###Epoch: 32\n",
      "Train loss 0.09000717324239237\n",
      "Valid loss 0.09409565904310771\n",
      "#################\n",
      "###Epoch: 33\n",
      "Train loss 0.0900073868257028\n",
      "Valid loss 0.09189947907413755\n",
      "#################\n",
      "###Epoch: 34\n",
      "Train loss 0.08918809283662725\n",
      "Valid loss 0.09581111477954048\n",
      "#################\n",
      "###Epoch: 35\n",
      "Train loss 0.08772576517528957\n",
      "Valid loss 0.0911278064761843\n",
      "#################\n",
      "###Epoch: 36\n",
      "Train loss 0.08674369366080673\n",
      "Valid loss 0.09469896235636302\n",
      "#################\n",
      "###Epoch: 37\n",
      "Train loss 0.08497136334578197\n",
      "Valid loss 0.08945624849625997\n",
      "#################\n",
      "###Epoch: 38\n",
      "Train loss 0.08313636702519876\n",
      "Valid loss 0.08835703879594803\n",
      "#################\n",
      "###Epoch: 39\n",
      "Train loss 0.08302383731912684\n",
      "Valid loss 0.0873298921755382\n",
      "#################\n",
      "###Epoch: 40\n",
      "Train loss 0.08124762525161107\n",
      "Valid loss 0.08825266999857766\n",
      "#################\n",
      "###Epoch: 41\n",
      "Train loss 0.08049698477541958\n",
      "Valid loss 0.08649373480251857\n",
      "#################\n",
      "###Epoch: 42\n",
      "Train loss 0.07902524068399712\n",
      "Valid loss 0.08477179067475456\n",
      "#################\n",
      "###Epoch: 43\n",
      "Train loss 0.07829296092192332\n",
      "Valid loss 0.08630336288894926\n",
      "#################\n",
      "###Epoch: 44\n",
      "Train loss 0.07768046800737027\n",
      "Valid loss 0.08376122266054153\n",
      "#################\n",
      "###Epoch: 45\n",
      "Train loss 0.07676266133785248\n",
      "Valid loss 0.08236511690276009\n",
      "#################\n",
      "###Epoch: 46\n",
      "Train loss 0.07489212423011109\n",
      "Valid loss 0.08424168399402074\n",
      "#################\n",
      "###Epoch: 47\n",
      "Train loss 0.07492019939753744\n",
      "Valid loss 0.0831085273197719\n",
      "#################\n",
      "###Epoch: 48\n",
      "Train loss 0.07329721638449917\n",
      "Valid loss 0.08061595154660088\n",
      "#################\n",
      "###Epoch: 49\n",
      "Train loss 0.07292425066784576\n",
      "Valid loss 0.08051777950354985\n",
      "#################\n",
      "###Epoch: 50\n",
      "Train loss 0.07265149774374785\n",
      "Valid loss 0.08140285845313754\n",
      "#################\n",
      "###Epoch: 51\n",
      "Train loss 0.07183482911851671\n",
      "Valid loss 0.08031089391027178\n",
      "#################\n",
      "###Epoch: 52\n",
      "Train loss 0.0710149232160162\n",
      "Valid loss 0.07987415471247264\n",
      "#################\n",
      "###Epoch: 53\n",
      "Train loss 0.07026497178055623\n",
      "Valid loss 0.07857120995010648\n",
      "#################\n",
      "###Epoch: 54\n",
      "Train loss 0.06942414630342413\n",
      "Valid loss 0.08020403555461339\n",
      "#################\n",
      "###Epoch: 55\n",
      "Train loss 0.06855576085271659\n",
      "Valid loss 0.07758879661560059\n",
      "#################\n",
      "###Epoch: 56\n",
      "Train loss 0.06864711283533662\n",
      "Valid loss 0.07858242626701083\n",
      "#################\n",
      "###Epoch: 57\n",
      "Train loss 0.06792349133778501\n",
      "Valid loss 0.07767095842531749\n",
      "#################\n",
      "###Epoch: 58\n",
      "Train loss 0.06630299502500782\n",
      "Valid loss 0.0769043458359582\n",
      "#################\n",
      "###Epoch: 59\n",
      "Train loss 0.06626775902178553\n",
      "Valid loss 0.07687397301197052\n",
      "#################\n",
      "###Epoch: 60\n",
      "Train loss 0.06534183163333822\n",
      "Valid loss 0.07625354081392288\n",
      "#################\n",
      "###Epoch: 61\n",
      "Train loss 0.06456540138633163\n",
      "Valid loss 0.07501232517617089\n",
      "#################\n",
      "###Epoch: 62\n",
      "Train loss 0.06390981376171112\n",
      "Valid loss 0.07630590775183269\n",
      "#################\n",
      "###Epoch: 63\n",
      "Train loss 0.06283791156278716\n",
      "Valid loss 0.07661644582237516\n",
      "#################\n",
      "###Epoch: 64\n",
      "Train loss 0.06293663603288156\n",
      "Valid loss 0.07577849818127495\n",
      "#################\n",
      "###Epoch: 65\n",
      "Train loss 0.06178213835314468\n",
      "Valid loss 0.07460243999958038\n",
      "#################\n",
      "###Epoch: 66\n",
      "Train loss 0.06134930087460412\n",
      "Valid loss 0.0739256399018424\n",
      "#################\n",
      "###Epoch: 67\n",
      "Train loss 0.06101486004061169\n",
      "Valid loss 0.07575005825076785\n",
      "#################\n",
      "###Epoch: 68\n",
      "Train loss 0.06039767160459801\n",
      "Valid loss 0.07481047511100769\n",
      "#################\n",
      "###Epoch: 69\n",
      "Train loss 0.06016670470988309\n",
      "Valid loss 0.0756845229438373\n",
      "#################\n",
      "###Epoch: 70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-f6e3efa606a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     fig = px.line(\n",
      "\u001b[0;32m<ipython-input-61-47eb6d87db90>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold, train_loader, valid_loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'###Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-1eea2f36cfeb>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(epoch, model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train different aggregators\n",
    "aggregator = \"mean\"\n",
    "config.gcn_agg = aggregator\n",
    "config.pretrain_dir = \"GCN_attention\"\n",
    "splits = KFold(n_splits=config.n_split, shuffle=True, random_state=config.seed).split(train_inputs)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "    train_dataset = TensorDataset(train_inputs[train_idx], train_adj[train_idx], train_labels[train_idx])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    valid_dataset = TensorDataset(train_inputs[val_idx], train_adj[val_idx], train_labels[val_idx])\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    train_losses, eval_losses = run(fold, train_loader, valid_loader)\n",
    "\n",
    "    fig = px.line(\n",
    "        pd.DataFrame([train_losses, eval_losses], index=['loss', 'val_loss']).T, \n",
    "        y=['loss', 'val_loss'], \n",
    "        labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n",
    "        title='Training History')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "public_df = test.query(\"seq_length == 107\").copy()\n",
    "public_inputs, public_adj = preprocess_inputs(public_df)\n",
    "public_inputs = torch.tensor(public_inputs, dtype=torch.long)\n",
    "public_adj = torch.tensor(public_adj, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using aggregate function mean\n"
     ]
    }
   ],
   "source": [
    "model_short = Net(seq_len=107, pred_len=107, K=config.K, aggregator=config.gcn_agg)\n",
    "\n",
    "list_public_preds = []\n",
    "for fold in range(5):\n",
    "    config.pretrain_dir = \"GCN_attention\"\n",
    "    model_short.load_state_dict(torch.load(f'{config.pretrain_dir}/gcn_gru_{fold}.pt'))\n",
    "    model_short.cuda()\n",
    "    model_short.eval()\n",
    "\n",
    "    public_preds = model_short(public_inputs.cuda(), public_adj.cuda())\n",
    "    public_preds = public_preds.cpu().detach().numpy()\n",
    "    \n",
    "    list_public_preds.append(public_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_df = test.query(\"seq_length == 130\").copy()\n",
    "private_inputs, private_adj = preprocess_inputs(private_df)\n",
    "private_inputs = torch.tensor(private_inputs, dtype=torch.long)\n",
    "private_adj = torch.tensor(private_adj, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using aggregate function mean\n"
     ]
    }
   ],
   "source": [
    "model_long = Net(seq_len=130, pred_len=130, K=config.K, aggregator=config.gcn_agg)\n",
    "list_private_preds = []\n",
    "private_dataset = TensorDataset(private_inputs,private_adj)\n",
    "private_loader = DataLoader(private_dataset, batch_size=16, shuffle=False, num_workers=8)\n",
    "\n",
    "for fold in range(5):\n",
    "    config.pretrain_dir = \"GCN_attention\"\n",
    "    model_long.load_state_dict(torch.load(f'{config.pretrain_dir}/gcn_gru_{fold}.pt'))\n",
    "    model_long.cuda()\n",
    "    model_long.eval()\n",
    "    batch_preds = []\n",
    "    for index, (private_inputs, private_adj) in enumerate(private_loader):\n",
    "        private_preds = model_long(private_inputs.cuda(), private_adj.cuda())\n",
    "        private_preds = private_preds.cpu().detach().numpy()\n",
    "\n",
    "        batch_preds.append(private_preds)\n",
    "\n",
    "    list_private_preds.append(np.concatenate(batch_preds,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_preds = np.mean(list_public_preds, axis=0)\n",
    "private_preds = np.mean(list_private_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ls = []\n",
    "\n",
    "for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n",
    "    for i, uid in enumerate(df.id):\n",
    "        single_pred = preds[i]\n",
    "\n",
    "        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n",
    "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "\n",
    "        preds_ls.append(single_df)\n",
    "\n",
    "preds_df = pd.concat(preds_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\n",
    "submission.to_csv('%s/submission.csv'%config.pretrain_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
