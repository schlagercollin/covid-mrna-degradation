{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    train_file = './stanford-covid-vaccine/train.json'\n",
    "    test_file = './stanford-covid-vaccine/test.json'\n",
    "    pretrain_dir = './GCN_lstm_patience10'\n",
    "    sample_submission = './stanford-covid-vaccine/sample_submission.csv'\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    n_epoch = 200\n",
    "    n_split = 5\n",
    "    K = 1 # number of aggregation loop (also means number of GCN layers)\n",
    "    gcn_agg = 'lstm' # aggregator function: mean, conv, lstm, pooling\n",
    "    filter_noise = True\n",
    "    patience= 10\n",
    "    seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    '''\n",
    "    Implementation of one layer of GraphSAGE\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, aggregator=config.gcn_agg):\n",
    "        super(GCN, self).__init__()\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "        if aggregator == 'mean':\n",
    "            linear_input_dim = input_dim * 2\n",
    "        elif aggregator == 'conv':\n",
    "            linear_input_dim = input_dim\n",
    "#         elif aggregator == 'pooling':\n",
    "#             linear_input_dim = input_dim * 2\n",
    "#             self.linear_pooling = nn.Linear(input_dim, input_dim)\n",
    "        elif aggregator == 'lstm':\n",
    "            self.lstm_hidden = 128\n",
    "            linear_input_dim = input_dim + self.lstm_hidden\n",
    "            self.lstm_agg = nn.LSTM(input_dim, self.lstm_hidden, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.linear_gcn = nn.Linear(in_features=linear_input_dim, out_features=output_dim)\n",
    "        \n",
    "    def forward(self, input_, adj_matrix):\n",
    "        if self.aggregator == 'conv':\n",
    "            # set elements in diagonal of adj matrix to 1 with conv aggregator\n",
    "            idx = torch.arange(0, adj_matrix.shape[-1], out=torch.LongTensor())\n",
    "            adj_matrix[:, idx, idx] = 1\n",
    "            \n",
    "        adj_matrix = adj_matrix.type(torch.float32)\n",
    "        sum_adj = torch.sum(adj_matrix, axis=2)\n",
    "        sum_adj[sum_adj==0] = 1\n",
    "        \n",
    "        if self.aggregator == 'mean' or self.aggregator == 'conv':\n",
    "            feature_agg = torch.bmm(adj_matrix, input_)\n",
    "            feature_agg = feature_agg / sum_adj.unsqueeze(dim=2)\n",
    "            \n",
    "#         elif self.aggregator == 'pooling':\n",
    "#             feature_pooling = self.linear_pooling(input_)\n",
    "#             feature_agg = torch.sigmoid(feature_pooling)\n",
    "#             feature_agg = torch.bmm(adj_matrix, feature_agg)\n",
    "#             feature_agg = feature_agg / sum_adj.unsqueeze(dim=2)\n",
    "\n",
    "        elif self.aggregator == 'lstm':\n",
    "            feature_agg = torch.zeros(input_.shape[0], input_.shape[1], self.lstm_hidden).cuda()\n",
    "            for i in range(adj_matrix.shape[1]):\n",
    "                neighbors = adj_matrix[:, i, :].unsqueeze(2) * input_\n",
    "                _, hn = self.lstm_agg(neighbors)\n",
    "                feature_agg[:, i, :] = torch.squeeze(hn[0], 0)\n",
    "                \n",
    "        if self.aggregator != 'conv':\n",
    "            feature_cat = torch.cat((input_, feature_agg), axis=2)\n",
    "        else:\n",
    "            feature_cat = feature_agg\n",
    "                \n",
    "        feature = torch.sigmoid(self.linear_gcn(feature_cat))\n",
    "        feature = feature / torch.norm(feature, p=2, dim=2).unsqueeze(dim=2)\n",
    "        \n",
    "        return feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_embedding=14, seq_len=107, pred_len=68, dropout=0.5, \n",
    "                 embed_dim=100, hidden_dim=128, K=1, aggregator='mean'):\n",
    "        '''\n",
    "        K: number of GCN layers\n",
    "        aggregator: type of aggregator function\n",
    "        '''\n",
    "        print(\"using aggregate function %s\"%aggregator)\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.pred_len = pred_len\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=num_embedding, \n",
    "                                      embedding_dim=embed_dim)\n",
    "        \n",
    "        self.gcn = nn.ModuleList([GCN(3 * embed_dim, 3 * embed_dim, aggregator=aggregator) for i in range(K)])\n",
    "        \n",
    "        self.gru_layer = nn.GRU(input_size=3 * embed_dim, \n",
    "                          hidden_size=hidden_dim, \n",
    "                          num_layers=3, \n",
    "                          batch_first=True, \n",
    "                          dropout=dropout, \n",
    "                          bidirectional=True)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=2 * hidden_dim, \n",
    "                                out_features=5)\n",
    "        \n",
    "    def forward(self, input_, adj_matrix):\n",
    "        #embedding\n",
    "        embedding = self.embedding_layer(input_)\n",
    "        embedding = torch.reshape(embedding, (-1, embedding.shape[1], embedding.shape[2] * embedding.shape[3]))\n",
    "        \n",
    "        #gcn\n",
    "        gcn_feature = embedding\n",
    "        for gcn_layer in self.gcn:\n",
    "            gcn_feature = gcn_layer(gcn_feature, adj_matrix)\n",
    "        \n",
    "        #gru\n",
    "        gru_output, gru_hidden = self.gru_layer(gcn_feature)\n",
    "        truncated = gru_output[:, :self.pred_len]\n",
    "        \n",
    "        output = self.linear_layer(truncated)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n",
    "\n",
    "def get_couples(structure):\n",
    "    \"\"\"\n",
    "    For each closing parenthesis, I find the matching opening one and store their index in the couples list.\n",
    "    The assigned list is used to keep track of the assigned opening parenthesis\n",
    "    \"\"\"\n",
    "    opened = [idx for idx, i in enumerate(structure) if i == '(']\n",
    "    closed = [idx for idx, i in enumerate(structure) if i == ')']\n",
    "\n",
    "    assert len(opened) == len(closed)\n",
    "    assigned = []\n",
    "    couples = []\n",
    "\n",
    "    for close_idx in closed:\n",
    "        for open_idx in opened:\n",
    "            if open_idx < close_idx:\n",
    "                if open_idx not in assigned:\n",
    "                    candidate = open_idx\n",
    "            else:\n",
    "                break\n",
    "        assigned.append(candidate)\n",
    "        couples.append([candidate, close_idx])\n",
    "        \n",
    "    assert len(couples) == len(opened)\n",
    "    \n",
    "    return couples\n",
    "\n",
    "def build_matrix(couples, size):\n",
    "    mat = np.zeros((size, size))\n",
    "    \n",
    "    for i in range(size):  # neigbouring bases are linked as well\n",
    "        if i < size - 1:\n",
    "            mat[i, i + 1] = 1\n",
    "        if i > 0:\n",
    "            mat[i, i - 1] = 1\n",
    "    \n",
    "    for i, j in couples:\n",
    "        mat[i, j] = 1\n",
    "        mat[j, i] = 1\n",
    "        \n",
    "    return mat\n",
    "\n",
    "def convert_to_adj(structure):\n",
    "    couples = get_couples(structure)\n",
    "    mat = build_matrix(couples, len(structure))\n",
    "    return mat\n",
    "\n",
    "def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n",
    "    inputs = np.transpose(\n",
    "        np.array(\n",
    "            df[cols]\n",
    "            .applymap(lambda seq: [token2int[x] for x in seq])\n",
    "            .values\n",
    "            .tolist()\n",
    "        ),\n",
    "        (0, 2, 1)\n",
    "    )\n",
    "    \n",
    "    adj_matrix = np.array(df['structure'].apply(convert_to_adj).values.tolist())\n",
    "    \n",
    "    return inputs, adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(config.train_file, lines=True)\n",
    "\n",
    "if config.filter_noise:\n",
    "    train = train[train.signal_to_noise > 1]\n",
    "    \n",
    "test = pd.read_json(config.test_file, lines=True)\n",
    "sample_df = pd.read_csv(config.sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_adj = preprocess_inputs(train)\n",
    "train_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.long)\n",
    "train_adj = torch.tensor(train_adj, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epoch, model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    train_loss = AverageMeter()\n",
    "    \n",
    "    for index, (input_, adj, label) in enumerate(train_loader):\n",
    "        input_ = input_.cuda()\n",
    "        adj = adj.cuda()\n",
    "        label = label.cuda()\n",
    "        preds = model(input_, adj)\n",
    "        \n",
    "        loss = criterion(preds, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.update(loss.item())\n",
    "    \n",
    "    print(f\"Train loss {train_loss.avg}\")\n",
    "    return train_loss.avg\n",
    "    \n",
    "def eval_fn(epoch, model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    eval_loss = AverageMeter()\n",
    "    \n",
    "    for index, (input_, adj, label) in enumerate(valid_loader):\n",
    "        input_ = input_.cuda()\n",
    "        adj = adj.cuda()\n",
    "        label = label.cuda()\n",
    "        preds = model(input_, adj)\n",
    "        \n",
    "        loss = criterion(preds, label)\n",
    "        eval_loss.update(loss.item())\n",
    "    \n",
    "    print(f\"Valid loss {eval_loss.avg}\")\n",
    "    return eval_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold, train_loader, valid_loader):\n",
    "    model = Net(K=config.K, aggregator=config.gcn_agg)\n",
    "    model.cuda()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=config.learning_rate, weight_decay=0.0)\n",
    "    \n",
    "    eval_loss_increase_step = 0\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    for epoch in range(config.n_epoch):\n",
    "        print('#################')\n",
    "        print('###Epoch:', epoch)\n",
    "        \n",
    "        train_loss = train_fn(epoch, model, train_loader, criterion, optimizer)\n",
    "        eval_loss = eval_fn(epoch, model, valid_loader, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        eval_losses.append(eval_loss)\n",
    "        \n",
    "        # check if should early stop\n",
    "        if len(eval_losses) == 2:\n",
    "            previous_eval_loss = eval_losses[0]\n",
    "        if len(eval_losses) >= 2:\n",
    "            if eval_loss > previous_eval_loss:\n",
    "                eval_loss_increase_step += 1\n",
    "            else: \n",
    "                # save the model if it is better than previous step\n",
    "                torch.save(model.state_dict(), f'{config.pretrain_dir}/gcn_gru_{fold}.pt')\n",
    "                eval_loss_increase_step = 0\n",
    "                previous_eval_loss=eval_loss\n",
    "                \n",
    "            print(\"previous_eval_loss %s\"%previous_eval_loss)\n",
    "        if eval_loss_increase_step >= config.patience:\n",
    "            print(\"early stop the model at Epoch: \", epoch)\n",
    "            del model\n",
    "            break\n",
    "            \n",
    "        \n",
    "#     torch.save(model.state_dict(), f'{config.pretrain_dir}/gcn_gru_{fold}.pt')\n",
    "    return train_losses, eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using aggregate function lstm\n",
      "#################\n",
      "###Epoch: 0\n",
      "Train loss 0.254132142773381\n",
      "Valid loss 0.22596994255270278\n",
      "#################\n",
      "###Epoch: 1\n",
      "Train loss 0.1843721651368671\n",
      "Valid loss 0.17508038878440857\n",
      "#################\n",
      "###Epoch: 2\n",
      "Train loss 0.1596649769279692\n",
      "Valid loss 0.15914849724088395\n",
      "previous_eval_loss 0.15914849724088395\n",
      "#################\n",
      "###Epoch: 3\n",
      "Train loss 0.14653262358020852\n",
      "Valid loss 0.14383844818387712\n",
      "previous_eval_loss 0.14383844818387712\n",
      "#################\n",
      "###Epoch: 4\n",
      "Train loss 0.13508761812139441\n",
      "Valid loss 0.1375142995800291\n",
      "previous_eval_loss 0.1375142995800291\n",
      "#################\n",
      "###Epoch: 5\n",
      "Train loss 0.12600838392972946\n",
      "Valid loss 0.13313844161374228\n",
      "previous_eval_loss 0.13313844161374228\n",
      "#################\n",
      "###Epoch: 6\n",
      "Train loss 0.12204641158933993\n",
      "Valid loss 0.12817504576274327\n",
      "previous_eval_loss 0.12817504576274327\n",
      "#################\n",
      "###Epoch: 7\n",
      "Train loss 0.11934932266120557\n",
      "Valid loss 0.1251163940344538\n",
      "previous_eval_loss 0.1251163940344538\n",
      "#################\n",
      "###Epoch: 8\n",
      "Train loss 0.11677266822920905\n",
      "Valid loss 0.12326131867510932\n",
      "previous_eval_loss 0.12326131867510932\n",
      "#################\n",
      "###Epoch: 9\n",
      "Train loss 0.11208389230348446\n",
      "Valid loss 0.12104155761854989\n",
      "previous_eval_loss 0.12104155761854989\n",
      "#################\n",
      "###Epoch: 10\n",
      "Train loss 0.10828518563950504\n",
      "Valid loss 0.11660995440823692\n",
      "previous_eval_loss 0.11660995440823692\n",
      "#################\n",
      "###Epoch: 11\n",
      "Train loss 0.10763904535108143\n",
      "Valid loss 0.11292276105710439\n",
      "previous_eval_loss 0.11292276105710439\n",
      "#################\n",
      "###Epoch: 12\n",
      "Train loss 0.10284407409252944\n",
      "Valid loss 0.10865938131298337\n",
      "previous_eval_loss 0.10865938131298337\n",
      "#################\n",
      "###Epoch: 13\n",
      "Train loss 0.09914445490748794\n",
      "Valid loss 0.1057115186538015\n",
      "previous_eval_loss 0.1057115186538015\n",
      "#################\n",
      "###Epoch: 14\n",
      "Train loss 0.09759669088655049\n",
      "Valid loss 0.10672173968383245\n",
      "previous_eval_loss 0.1057115186538015\n",
      "#################\n",
      "###Epoch: 15\n",
      "Train loss 0.09491639225571244\n",
      "Valid loss 0.10257120324032647\n",
      "previous_eval_loss 0.10257120324032647\n",
      "#################\n",
      "###Epoch: 16\n",
      "Train loss 0.09262637104149218\n",
      "Valid loss 0.09872440887348992\n",
      "previous_eval_loss 0.09872440887348992\n",
      "#################\n",
      "###Epoch: 17\n",
      "Train loss 0.09099695058884444\n",
      "Valid loss 0.09649306855031423\n",
      "previous_eval_loss 0.09649306855031423\n",
      "#################\n",
      "###Epoch: 18\n",
      "Train loss 0.08742884270570896\n",
      "Valid loss 0.09360546405826296\n",
      "previous_eval_loss 0.09360546405826296\n",
      "#################\n",
      "###Epoch: 19\n",
      "Train loss 0.08507201765422467\n",
      "Valid loss 0.09171362966299057\n",
      "previous_eval_loss 0.09171362966299057\n",
      "#################\n",
      "###Epoch: 20\n",
      "Train loss 0.08423776510688993\n",
      "Valid loss 0.09380500550780978\n",
      "previous_eval_loss 0.09171362966299057\n",
      "#################\n",
      "###Epoch: 21\n",
      "Train loss 0.08191137043414293\n",
      "Valid loss 0.08859895489045552\n",
      "previous_eval_loss 0.08859895489045552\n",
      "#################\n",
      "###Epoch: 22\n",
      "Train loss 0.07958356539408366\n",
      "Valid loss 0.0858456918171474\n",
      "previous_eval_loss 0.0858456918171474\n",
      "#################\n",
      "###Epoch: 23\n",
      "Train loss 0.07930942679996844\n",
      "Valid loss 0.0879831952708108\n",
      "previous_eval_loss 0.0858456918171474\n",
      "#################\n",
      "###Epoch: 24\n",
      "Train loss 0.07891792003755216\n",
      "Valid loss 0.08517500332423619\n",
      "previous_eval_loss 0.08517500332423619\n",
      "#################\n",
      "###Epoch: 25\n",
      "Train loss 0.07641655565411956\n",
      "Valid loss 0.08327652088233403\n",
      "previous_eval_loss 0.08327652088233403\n",
      "#################\n",
      "###Epoch: 26\n",
      "Train loss 0.07521429437178152\n",
      "Valid loss 0.08303488578115191\n",
      "previous_eval_loss 0.08303488578115191\n",
      "#################\n",
      "###Epoch: 27\n",
      "Train loss 0.07228900795733487\n",
      "Valid loss 0.08070638030767441\n",
      "previous_eval_loss 0.08070638030767441\n",
      "#################\n",
      "###Epoch: 28\n",
      "Train loss 0.07212041635756139\n",
      "Valid loss 0.07998841468776975\n",
      "previous_eval_loss 0.07998841468776975\n",
      "#################\n",
      "###Epoch: 29\n",
      "Train loss 0.07137801070456151\n",
      "Valid loss 0.08231888285705022\n",
      "previous_eval_loss 0.07998841468776975\n",
      "#################\n",
      "###Epoch: 30\n",
      "Train loss 0.07036477630888974\n",
      "Valid loss 0.07841863589627403\n",
      "previous_eval_loss 0.07841863589627403\n",
      "#################\n",
      "###Epoch: 31\n",
      "Train loss 0.06941905369361241\n",
      "Valid loss 0.07922182338578361\n",
      "previous_eval_loss 0.07841863589627403\n",
      "#################\n",
      "###Epoch: 32\n",
      "Train loss 0.0681189880878837\n",
      "Valid loss 0.07908554268734795\n",
      "previous_eval_loss 0.07841863589627403\n",
      "#################\n",
      "###Epoch: 33\n",
      "Train loss 0.06732441346954417\n",
      "Valid loss 0.076975160411426\n",
      "previous_eval_loss 0.076975160411426\n",
      "#################\n",
      "###Epoch: 34\n",
      "Train loss 0.06642793174143191\n",
      "Valid loss 0.07579497354371208\n",
      "previous_eval_loss 0.07579497354371208\n",
      "#################\n",
      "###Epoch: 35\n",
      "Train loss 0.06657350532434604\n",
      "Valid loss 0.07763011753559113\n",
      "previous_eval_loss 0.07579497354371208\n",
      "#################\n",
      "###Epoch: 36\n",
      "Train loss 0.06571487627095646\n",
      "Valid loss 0.07764745610100883\n",
      "previous_eval_loss 0.07579497354371208\n",
      "#################\n",
      "###Epoch: 37\n",
      "Train loss 0.06413646511457584\n",
      "Valid loss 0.07584822071450097\n",
      "previous_eval_loss 0.07579497354371208\n",
      "#################\n",
      "###Epoch: 38\n",
      "Train loss 0.06498268177663838\n",
      "Valid loss 0.0754528705562864\n",
      "previous_eval_loss 0.0754528705562864\n",
      "#################\n",
      "###Epoch: 39\n",
      "Train loss 0.06416821424607877\n",
      "Valid loss 0.07602959126234055\n",
      "previous_eval_loss 0.0754528705562864\n",
      "#################\n",
      "###Epoch: 40\n",
      "Train loss 0.06329049280396214\n",
      "Valid loss 0.07698217353650502\n",
      "previous_eval_loss 0.0754528705562864\n",
      "#################\n",
      "###Epoch: 41\n",
      "Train loss 0.06294939915339152\n",
      "Valid loss 0.07475502469709941\n",
      "previous_eval_loss 0.07475502469709941\n",
      "#################\n",
      "###Epoch: 42\n",
      "Train loss 0.0620214830118197\n",
      "Valid loss 0.07383694499731064\n",
      "previous_eval_loss 0.07383694499731064\n",
      "#################\n",
      "###Epoch: 43\n",
      "Train loss 0.06128147754956175\n",
      "Valid loss 0.07306017088038581\n",
      "previous_eval_loss 0.07306017088038581\n",
      "#################\n",
      "###Epoch: 44\n",
      "Train loss 0.06107427207408128\n",
      "Valid loss 0.07300273861203875\n",
      "previous_eval_loss 0.07300273861203875\n",
      "#################\n",
      "###Epoch: 45\n",
      "Train loss 0.060451218375453246\n",
      "Valid loss 0.07327982464006969\n",
      "previous_eval_loss 0.07300273861203875\n",
      "#################\n",
      "###Epoch: 46\n",
      "Train loss 0.06010354241287267\n",
      "Valid loss 0.07231477754456657\n",
      "previous_eval_loss 0.07231477754456657\n",
      "#################\n",
      "###Epoch: 47\n",
      "Train loss 0.05921614542603493\n",
      "Valid loss 0.07295203741107668\n",
      "previous_eval_loss 0.07231477754456657\n",
      "#################\n",
      "###Epoch: 48\n",
      "Train loss 0.0585829161659435\n",
      "Valid loss 0.0713084020784923\n",
      "previous_eval_loss 0.0713084020784923\n",
      "#################\n",
      "###Epoch: 49\n",
      "Train loss 0.05878961348423251\n",
      "Valid loss 0.07160755779061999\n",
      "previous_eval_loss 0.0713084020784923\n",
      "#################\n",
      "###Epoch: 50\n",
      "Train loss 0.058145667943689555\n",
      "Valid loss 0.0718814241034644\n",
      "previous_eval_loss 0.0713084020784923\n",
      "#################\n",
      "###Epoch: 51\n",
      "Train loss 0.05772814265003911\n",
      "Valid loss 0.07394805337701525\n",
      "previous_eval_loss 0.0713084020784923\n",
      "#################\n",
      "###Epoch: 52\n",
      "Train loss 0.057252870665656194\n",
      "Valid loss 0.0719306543469429\n",
      "previous_eval_loss 0.0713084020784923\n",
      "#################\n",
      "###Epoch: 53\n",
      "Train loss 0.05639912160458388\n",
      "Valid loss 0.07152200277362551\n",
      "previous_eval_loss 0.0713084020784923\n",
      "#################\n",
      "###Epoch: 54\n",
      "Train loss 0.055762413475248546\n",
      "Valid loss 0.0707492083311081\n",
      "previous_eval_loss 0.0707492083311081\n",
      "#################\n",
      "###Epoch: 55\n",
      "Train loss 0.05633778125047684\n",
      "Valid loss 0.0711316242814064\n",
      "previous_eval_loss 0.0707492083311081\n",
      "#################\n",
      "###Epoch: 56\n",
      "Train loss 0.05584098771214485\n",
      "Valid loss 0.06951933673449925\n",
      "previous_eval_loss 0.06951933673449925\n",
      "#################\n",
      "###Epoch: 57\n",
      "Train loss 0.05515647210456707\n",
      "Valid loss 0.07137910010559219\n",
      "previous_eval_loss 0.06951933673449925\n",
      "#################\n",
      "###Epoch: 58\n",
      "Train loss 0.05547385483428284\n",
      "Valid loss 0.071063724479505\n",
      "previous_eval_loss 0.06951933673449925\n",
      "#################\n",
      "###Epoch: 59\n"
     ]
    }
   ],
   "source": [
    "# train different aggregators\n",
    "aggregators = [\"lstm\"]\n",
    "for aggregator in aggregators:\n",
    "    config.gcn_agg = aggregator\n",
    "    config.pretrain_dir = \"GCN_%s_patience10\"%aggregator\n",
    "    splits = KFold(n_splits=config.n_split, shuffle=True, random_state=config.seed).split(train_inputs)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        train_dataset = TensorDataset(train_inputs[train_idx], train_adj[train_idx], train_labels[train_idx])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "        valid_dataset = TensorDataset(train_inputs[val_idx], train_adj[val_idx], train_labels[val_idx])\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "        train_losses, eval_losses = run(fold, train_loader, valid_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    pd.DataFrame([train_losses, eval_losses], index=['loss', 'val_loss']).T, \n",
    "    y=['loss', 'val_loss'], \n",
    "    labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n",
    "    title='Training History')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_df = test.query(\"seq_length == 107\").copy()\n",
    "private_df = test.query(\"seq_length == 130\").copy()\n",
    "\n",
    "public_inputs, public_adj = preprocess_inputs(public_df)\n",
    "private_inputs, private_adj = preprocess_inputs(private_df)\n",
    "\n",
    "public_inputs = torch.tensor(public_inputs, dtype=torch.long)\n",
    "private_inputs = torch.tensor(private_inputs, dtype=torch.long)\n",
    "public_adj = torch.tensor(public_adj, dtype=torch.long)\n",
    "private_adj = torch.tensor(private_adj, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using aggregate function mean\n",
      "using aggregate function mean\n"
     ]
    }
   ],
   "source": [
    "model_short = Net(seq_len=107, pred_len=107, K=config.K, aggregator=config.gcn_agg)\n",
    "model_long = Net(seq_len=130, pred_len=130, K=config.K, aggregator=config.gcn_agg)\n",
    "\n",
    "list_public_preds = []\n",
    "list_private_preds = []\n",
    "for fold in range(config.n_split):\n",
    "    model_short.load_state_dict(torch.load(f'{config.pretrain_dir}/gcn_gru_{fold}.pt'))\n",
    "    model_long.load_state_dict(torch.load(f'{config.pretrain_dir}/gcn_gru_{fold}.pt'))\n",
    "    model_short.cuda()\n",
    "    model_long.cuda()\n",
    "    model_short.eval()\n",
    "    model_long.eval()\n",
    "\n",
    "    public_preds = model_short(public_inputs.cuda(), public_adj.cuda())\n",
    "    private_preds = model_long(private_inputs.cuda(), private_adj.cuda())\n",
    "    public_preds = public_preds.cpu().detach().numpy()\n",
    "    private_preds = private_preds.cpu().detach().numpy()\n",
    "    \n",
    "    list_public_preds.append(public_preds)\n",
    "    list_private_preds.append(private_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_preds = np.mean(list_public_preds, axis=0)\n",
    "private_preds = np.mean(list_private_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ls = []\n",
    "\n",
    "for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n",
    "    for i, uid in enumerate(df.id):\n",
    "        single_pred = preds[i]\n",
    "\n",
    "        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n",
    "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "\n",
    "        preds_ls.append(single_df)\n",
    "\n",
    "preds_df = pd.concat(preds_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\n",
    "submission.to_csv('%s/submission.csv'%config.pretrain_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
