{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ko5-LSj-fMvx",
    "papermill": {
     "duration": 0.014398,
     "end_time": "2020-09-17T02:35:46.102764",
     "exception": false,
     "start_time": "2020-09-17T02:35:46.088366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T02:35:46.139235Z",
     "iopub.status.busy": "2020-09-17T02:35:46.138457Z",
     "iopub.status.idle": "2020-09-17T02:35:49.156939Z",
     "shell.execute_reply": "2020-09-17T02:35:49.156444Z"
    },
    "id": "-Ci0Ma0BfMvy",
    "papermill": {
     "duration": 3.039526,
     "end_time": "2020-09-17T02:35:49.157082",
     "exception": false,
     "start_time": "2020-09-17T02:35:46.117556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/veer/covid-mrna-degradation'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import copy\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ib-9gDOzfMv1",
    "papermill": {
     "duration": 0.014705,
     "end_time": "2020-09-17T02:35:49.187072",
     "exception": false,
     "start_time": "2020-09-17T02:35:49.172367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T02:35:49.224195Z",
     "iopub.status.busy": "2020-09-17T02:35:49.223544Z",
     "iopub.status.idle": "2020-09-17T02:35:49.227413Z",
     "shell.execute_reply": "2020-09-17T02:35:49.227001Z"
    },
    "id": "CAMfRo5ifMv2",
    "papermill": {
     "duration": 0.024218,
     "end_time": "2020-09-17T02:35:49.227502",
     "exception": false,
     "start_time": "2020-09-17T02:35:49.203284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    train_file = '/home/veer/covid-mrna-degradation/data/train.json'\n",
    "    test_file = '/home/veer/covid-mrna-degradation/data/test.json'\n",
    "    pretrain_dir = './'\n",
    "    sample_submission = '/home/veer/covid-mrna-degradation/data/sample_submission.csv'\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    n_epoch = 100\n",
    "    n_split = 2\n",
    "    K = 1 # number of aggregation loop (also means number of GCN layers)\n",
    "    gcn_agg = 'mean' # aggregator function: mean, conv, lstm, pooling\n",
    "    filter_noise = True\n",
    "    seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyUT0vsEfMwD",
    "papermill": {
     "duration": 0.014981,
     "end_time": "2020-09-17T02:35:49.473080",
     "exception": false,
     "start_time": "2020-09-17T02:35:49.458099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T02:35:49.507398Z",
     "iopub.status.busy": "2020-09-17T02:35:49.506717Z",
     "iopub.status.idle": "2020-09-17T02:35:49.510442Z",
     "shell.execute_reply": "2020-09-17T02:35:49.509934Z"
    },
    "id": "WyGKPKapfMwE",
    "papermill": {
     "duration": 0.022267,
     "end_time": "2020-09-17T02:35:49.510525",
     "exception": false,
     "start_time": "2020-09-17T02:35:49.488258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T02:35:49.556772Z",
     "iopub.status.busy": "2020-09-17T02:35:49.556099Z",
     "iopub.status.idle": "2020-09-17T02:35:49.558941Z",
     "shell.execute_reply": "2020-09-17T02:35:49.558500Z"
    },
    "id": "_GV2vZi5fMwH",
    "papermill": {
     "duration": 0.033215,
     "end_time": "2020-09-17T02:35:49.559047",
     "exception": false,
     "start_time": "2020-09-17T02:35:49.525832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n",
    "\n",
    "def get_couples(structure):\n",
    "    \"\"\"\n",
    "    For each closing parenthesis, I find the matching opening one and store their index in the couples list.\n",
    "    The assigned list is used to keep track of the assigned opening parenthesis\n",
    "    \"\"\"\n",
    "    opened = [idx for idx, i in enumerate(structure) if i == '(']\n",
    "    closed = [idx for idx, i in enumerate(structure) if i == ')']\n",
    "\n",
    "    assert len(opened) == len(closed)\n",
    "    assigned = []\n",
    "    couples = []\n",
    "\n",
    "    for close_idx in closed:\n",
    "        for open_idx in opened:\n",
    "            if open_idx < close_idx:\n",
    "                if open_idx not in assigned:\n",
    "                    candidate = open_idx\n",
    "            else:\n",
    "                break\n",
    "        assigned.append(candidate)\n",
    "        couples.append([candidate, close_idx])\n",
    "        \n",
    "    assert len(couples) == len(opened)\n",
    "    \n",
    "    return couples\n",
    "\n",
    "def build_matrix(couples, size):\n",
    "    mat = np.zeros((size, size))\n",
    "    \n",
    "    for i in range(size):  # neigbouring bases are linked as well\n",
    "        if i < size - 1:\n",
    "            mat[i, i + 1] = 1\n",
    "        if i > 0:\n",
    "            mat[i, i - 1] = 1\n",
    "    \n",
    "    for i, j in couples:\n",
    "        mat[i, j] = 1\n",
    "        mat[j, i] = 1\n",
    "        \n",
    "    return mat\n",
    "\n",
    "def convert_to_adj(structure):\n",
    "    couples = get_couples(structure)\n",
    "    mat = build_matrix(couples, len(structure))\n",
    "    return mat\n",
    "\n",
    "def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n",
    "    inputs = np.transpose(\n",
    "        np.array(\n",
    "            df[cols]\n",
    "            .applymap(lambda seq: [token2int[x] for x in seq])\n",
    "            .values\n",
    "            .tolist()\n",
    "        ),\n",
    "        (0, 2, 1)\n",
    "    )\n",
    "    \n",
    "    adj_matrix = np.array(df['structure'].apply(convert_to_adj).values.tolist())\n",
    "    \n",
    "    return inputs, adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T02:35:49.605738Z",
     "iopub.status.busy": "2020-09-17T02:35:49.601132Z",
     "iopub.status.idle": "2020-09-17T02:35:50.806540Z",
     "shell.execute_reply": "2020-09-17T02:35:50.806010Z"
    },
    "id": "6zuAYCKrfMwK",
    "papermill": {
     "duration": 1.231464,
     "end_time": "2020-09-17T02:35:50.806651",
     "exception": false,
     "start_time": "2020-09-17T02:35:49.575187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(config.train_file, lines=True)\n",
    "\n",
    "if config.filter_noise:\n",
    "    train = train[train.signal_to_noise > 1]\n",
    "    \n",
    "test = pd.read_json(config.test_file, lines=True)\n",
    "sample_df = pd.read_csv(config.sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T02:35:50.849189Z",
     "iopub.status.busy": "2020-09-17T02:35:50.848222Z",
     "iopub.status.idle": "2020-09-17T02:35:51.790209Z",
     "shell.execute_reply": "2020-09-17T02:35:51.789690Z"
    },
    "id": "1JCDE6lBfMwM",
    "papermill": {
     "duration": 0.967438,
     "end_time": "2020-09-17T02:35:51.790309",
     "exception": false,
     "start_time": "2020-09-17T02:35:50.822871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO ALL THIS BEFORE THE PERTURBATIONS:\n",
    "base_pairs = {3:'A', 4:'C', 5:'G', 6:'U'}\n",
    "\n",
    "train_inputs, train_adj = preprocess_inputs(train)\n",
    "train_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))\n",
    "\n",
    "num_seqs = train_inputs.shape[0]\n",
    "seq_len = train_inputs.shape[1]\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.long)\n",
    "train_adj = torch.tensor(train_adj, dtype=torch.float32, requires_grad=False)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
    "\n",
    "#embedding\n",
    "embedding_layer = nn.Embedding(num_embeddings=14, \n",
    "                                      embedding_dim=100)\n",
    "\n",
    "results_dict = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    '''\n",
    "    Implementation of one layer of GraphSAGE\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, aggregator='mean'):\n",
    "        super(GCN, self).__init__()\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "        if aggregator == 'mean':\n",
    "            linear_input_dim = input_dim * 2\n",
    "        elif aggregator == 'conv':\n",
    "            linear_input_dim = input_dim\n",
    "        elif aggregator == 'pooling':\n",
    "            linear_input_dim = input_dim * 2\n",
    "            self.linear_pooling = nn.Linear(input_dim, input_dim)\n",
    "        elif aggregator == 'lstm':\n",
    "            self.lstm_hidden = 128\n",
    "            linear_input_dim = input_dim + self.lstm_hidden\n",
    "            self.lstm_agg = nn.LSTM(input_dim, self.lstm_hidden, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.linear_gcn = nn.Linear(in_features=linear_input_dim, out_features=output_dim)\n",
    "        \n",
    "    def forward(self, input_, adj_matrix):\n",
    "        if self.aggregator == 'conv':\n",
    "            # set elements in diagonal of adj matrix to 1 with conv aggregator\n",
    "            idx = torch.arange(0, adj_matrix.shape[-1], out=torch.LongTensor())\n",
    "            adj_matrix[:, idx, idx] = 1\n",
    "            \n",
    "        adj_matrix = adj_matrix.type(torch.float32)\n",
    "        sum_adj = torch.sum(adj_matrix, axis=2)\n",
    "        sum_adj[sum_adj==0] = 1\n",
    "        \n",
    "        if self.aggregator == 'mean' or self.aggregator == 'conv':\n",
    "            feature_agg = torch.bmm(adj_matrix, input_)\n",
    "            feature_agg = feature_agg / sum_adj.unsqueeze(dim=2)\n",
    "            \n",
    "        elif self.aggregator == 'pooling':\n",
    "            feature_pooling = self.linear_pooling(input_)\n",
    "            feature_agg = torch.sigmoid(feature_pooling)\n",
    "            feature_agg = torch.bmm(adj_matrix, feature_agg)\n",
    "            feature_agg = feature_agg / sum_adj.unsqueeze(dim=2)\n",
    "\n",
    "        elif self.aggregator == 'lstm':\n",
    "            feature_agg = torch.zeros(input_.shape[0], input_.shape[1], self.lstm_hidden)\n",
    "            for i in range(adj_matrix.shape[1]):\n",
    "                neighbors = adj_matrix[:, i, :].unsqueeze(2) * input_\n",
    "                _, hn = self.lstm_agg(neighbors)\n",
    "                feature_agg[:, i, :] = torch.squeeze(hn[0], 0)\n",
    "                \n",
    "        if self.aggregator != 'conv':\n",
    "            feature_cat = torch.cat((input_, feature_agg), axis=2)\n",
    "        else:\n",
    "            feature_cat = feature_agg\n",
    "                \n",
    "        feature = torch.sigmoid(self.linear_gcn(feature_cat))\n",
    "        feature = feature / torch.norm(feature, p=2, dim=2).unsqueeze(dim=2)\n",
    "        \n",
    "        return feature\n",
    "        \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_embedding=14, seq_len=107, pred_len=68, dropout=0.5, \n",
    "                 embed_dim=100, hidden_dim=128, K=1, aggregator='mean'):\n",
    "        '''\n",
    "        K: number of GCN layers\n",
    "        aggregator: type of aggregator function\n",
    "        '''\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        self.gcn = nn.ModuleList([GCN(3 * embed_dim, 3 * embed_dim, aggregator=aggregator) for i in range(K)])\n",
    "        \n",
    "        self.rnn_layer = nn.LSTM(input_size=3 * embed_dim, \n",
    "                          hidden_size=hidden_dim, \n",
    "                          num_layers=3, \n",
    "                          batch_first=True, \n",
    "                          dropout=dropout, \n",
    "                          bidirectional=True)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=2 * hidden_dim, \n",
    "                                out_features=1)\n",
    "        \n",
    "    def forward(self, input_, adj_matrix):\n",
    "        #gcn\n",
    "        gcn_feature = input_\n",
    "        for gcn_layer in self.gcn:\n",
    "            gcn_feature = gcn_layer(gcn_feature, adj_matrix)\n",
    "        \n",
    "        #rnn\n",
    "        rnn_output, rnn_hidden = self.rnn_layer(gcn_feature)\n",
    "        truncated = rnn_output[:, :self.pred_len]\n",
    "        \n",
    "        output = self.linear_layer(truncated)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/baseline_model/baseline_4.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-40272350bb28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m107\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m107\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn_agg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/baseline_model/baseline_4.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/baseline_model/baseline_4.pt'"
     ]
    }
   ],
   "source": [
    "# EXECUTE THE PERTURBATIONS\n",
    "\n",
    "## Load the model\n",
    "model = Net(seq_len=107, pred_len=107, K=config.K, aggregator=config.gcn_agg)\n",
    "model.load_state_dict(torch.load('/home/veer/covid-mrna-degradation'))\n",
    "\n",
    "for i in range(seq_len):\n",
    "    orig = copy.deepcopy(train_inputs[:,i,0])\n",
    "    results_dict[i] = {}\n",
    "    \n",
    "    for base in [3,4,5,6]:\n",
    "        #print(\"Base = {}; Sequence Position = {}\".format(base_pairs[base], i))\n",
    "        train_inputs[:,i,0] = base\n",
    "        \n",
    "        # Convert to embedding and get it into training format\n",
    "        train_inputs_tens = torch.tensor(train_inputs, dtype=torch.long)\n",
    "        train_inputs_tens = embedding_layer(train_inputs_tens)\n",
    "        train_inputs_tens = \\\n",
    "        torch.reshape(train_inputs_tens, (-1, train_inputs_tens.shape[1], train_inputs_tens.shape[2] * train_inputs_tens.shape[3]))\n",
    "        train_inputs_tens = train_inputs_tens.clone().detach().requires_grad_(False)\n",
    "        \n",
    "        # DO FORWARD PASS. (forward pass function)\n",
    "        # results_dict[i][base_pairs[base]] = results list of 5(?)\n",
    "        model.eval()\n",
    "        preds = model(train_inputs_tens, train_adj)\n",
    "        preds = public_preds.cpu().detach().numpy()\n",
    "        \n",
    "        results_dict[i][base_pairs[base]] = preds\n",
    "        \n",
    "        \n",
    "    train_inputs[:,i,0] = orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/veer/covid-mrna-degradation'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "rna-degradation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "duration": 1310.426235,
   "end_time": "2020-09-17T02:57:32.292019",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-17T02:35:41.865784",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
